name: Performance Optimization

on:
  schedule:
    - cron: '0 0 15 * *'  # Monthly on the 15th
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write
  issues: write

jobs:
  optimize:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Setup with mise
      uses: jdx/mise-action@v2
      with:
        cache: true
        experimental: true
        
    - name: Install dependencies and tools
      run: |
        # Install local dependencies first
        npm ci || npm install
        # Install Python tools
        pip install -q aider-chat
        # Install globally
        npm install -g .

    - name: Profile Current Performance
      run: |
        # Create performance test
        cat > perf-test.js << 'EOF'
        import { spawn } from 'child_process';
        import { performance } from 'perf_hooks';
        
        const scenarios = [
          { name: 'spawn-burst', count: 100, cmd: ['echo', 'test'] },
          { name: 'fd-search', count: 10, cmd: ['fd', '.js', '-t', 'f'] },
          { name: 'grep-heavy', count: 20, cmd: ['grep', '-r', 'TODO', '.'] }
        ];
        
        for (const scenario of scenarios) {
          const start = performance.now();
          const promises = [];
          
          for (let i = 0; i < scenario.count; i++) {
            promises.push(new Promise((resolve) => {
              const proc = spawn(scenario.cmd[0], scenario.cmd.slice(1));
              proc.on('close', resolve);
            }));
          }
          
          await Promise.all(promises);
          const time = performance.now() - start;
          console.log(`${scenario.name}: ${time.toFixed(2)}ms (${(time/scenario.count).toFixed(2)}ms avg)`);
        }
        EOF
        
        echo "### Baseline Performance:" > performance-baseline.txt
        timeout 60 node perf-test.js >> performance-baseline.txt 2>&1
        
        echo "### With claude-yolt:" >> performance-baseline.txt
        timeout 60 node --require ./lib/process-interceptor.js perf-test.js >> performance-baseline.txt 2>&1

    - name: Optimization Analysis
      env:
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        if [ -n "$GITHUB_TOKEN" ]; then
          curl -s "https://models.inference.ai.azure.com/chat/completions" \
            -H "Authorization: Bearer $GITHUB_TOKEN" \
            -H "Content-Type: application/json" \
            -d '{
              "model": "gpt-4o-2024-11-20",
              "messages": [{
                "role": "user",
                "content": "Analyze performance optimization opportunities in claude-yolt:\n\nCurrent performance:\n'$(cat performance-baseline.txt | jq -Rs .)'\n\nAnalyze components for optimization:\n1. Process queue implementation\n2. Stream handling efficiency\n3. Event emitter usage\n4. Map/Set operations\n5. String operations\n6. Async/await patterns\n\nProvide:\n- Specific optimizations with code\n- Expected performance gain\n- Complexity improvement (O notation)\n- Memory usage reduction"
              }],
              "temperature": 0.1,
              "max_tokens": 3000
            }' | jq -r '.choices[0].message.content' > optimization-report.md
        elif [ -n "$OPENROUTER_API_KEY" ]; then
          aider --model nousresearch/hermes-3-llama-3.1-405b:free \
                --message "Analyze performance optimization opportunities: process queue, stream handling, event emitters, Map/Set ops, async patterns. Include specific code optimizations and O notation analysis" \
                lib/process-interceptor.js --no-git --yes > optimization-report.md
        else
          echo "## Performance Optimization Analysis" > optimization-report.md
          echo "Analysis skipped - no API key available" >> optimization-report.md
        fi

    - name: Generate Optimized Code
      env:
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        if [ -n "$OPENROUTER_API_KEY" ]; then
          aider --model qwen/qwen-2.5-coder-32b-instruct:free \
                --message "Implement the most impactful optimization from the analysis. Generate optimized version, benchmark to prove improvement, unit test for optimization" \
                lib/process-interceptor.js --no-git --yes > optimized-code.js
        elif [ -n "$GITHUB_TOKEN" ]; then
          curl -s "https://models.inference.ai.azure.com/chat/completions" \
            -H "Authorization: Bearer $GITHUB_TOKEN" \
            -H "Content-Type: application/json" \
            -d '{
              "model": "gpt-4o-2024-11-20",
              "messages": [{
                "role": "user",
                "content": "Based on the previous analysis, implement the most impactful optimization:\n\n1. Optimized version of the specific function/component\n2. Benchmark to prove improvement\n3. Unit test for the optimization\n\nOutput the complete optimized code"
              }],
              "temperature": 0.1,
              "max_tokens": 2500
            }' | jq -r '.choices[0].message.content' > optimized-code.js
        else
          echo "## Optimized Code" > optimized-code.js
          echo "Code generation skipped - no API key available" >> optimized-code.js
        fi

    - name: Memory Optimization
      env:
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        if [ -n "$OPENROUTER_API_KEY" ]; then
          aider --model deepseek/deepseek-r1:free \
                --message "Optimize memory usage: Maps->WeakMaps, Arrays->Sets, string concat->buffers, avoid object cloning, reduce closures" \
                lib/ --no-git --yes >> optimization-report.md
        elif [ -n "$GITHUB_TOKEN" ]; then
          curl -s "https://models.inference.ai.azure.com/chat/completions" \
            -H "Authorization: Bearer $GITHUB_TOKEN" \
            -H "Content-Type: application/json" \
            -d '{
              "model": "gpt-4o-mini",
              "messages": [{
                "role": "user",
                "content": "Optimize memory usage in claude-yolt:\n\n1. Maps that could be WeakMaps\n2. Arrays that could be Sets\n3. String concatenation vs buffers\n4. Unnecessary object cloning\n5. Large closures\n\nSuggest memory-efficient alternatives"
              }],
              "temperature": 0.1,
              "max_tokens": 1500
            }' | jq -r '.choices[0].message.content' >> optimization-report.md
        else
          echo "## Memory Optimization" >> optimization-report.md
          echo "Analysis skipped - no API key available" >> optimization-report.md
        fi

    - name: Algorithm Optimization
      env:
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        if [ -n "$GITHUB_TOKEN" ]; then
          curl -s "https://models.inference.ai.azure.com/chat/completions" \
            -H "Authorization: Bearer $GITHUB_TOKEN" \
            -H "Content-Type: application/json" \
            -d '{
              "model": "gpt-4o-2024-11-20",
              "messages": [{
                "role": "user",
                "content": "Find algorithm improvements in claude-yolt:\n\n1. Linear searches that could be O(1) with Map\n2. Nested loops that could be flattened\n3. Repeated calculations without memoization\n4. Inefficient queue operations\n5. Suboptimal data structures\n\nFocus on hot paths: trySpawnNext() in process queue, Stream data handling, Process cleanup loops\n\nProvide Big O analysis for current vs optimized"
              }],
              "temperature": 0.1,
              "max_tokens": 2000
            }' | jq -r '.choices[0].message.content' >> optimization-report.md
        elif [ -n "$OPENROUTER_API_KEY" ]; then
          aider --model nousresearch/hermes-3-llama-3.1-405b:free \
                --message "Find algorithm improvements: linear->O(1) searches, flatten nested loops, memoization, efficient queues. Focus on trySpawnNext(), stream handling, cleanup. Include Big O analysis" \
                lib/process-interceptor.js --no-git --yes >> optimization-report.md
        else
          echo "## Algorithm Optimization" >> optimization-report.md
          echo "Analysis skipped - no API key available" >> optimization-report.md
        fi

    - name: Create Optimization PR
      run: |
        # Check if we have valid optimizations
        if ! grep -q "Expected performance gain\|O(" optimization-report.md; then
          echo "No clear optimizations found"
          exit 0
        fi
        
        # Create optimization branch
        BRANCH="perf/optimize-$(date +%Y%m%d)"
        git checkout -b $BRANCH
        
        # If we have optimized code, add it
        if [ -f optimized-code.js ] && [ -s optimized-code.js ]; then
          # This is a demo - in reality we'd integrate the optimized code properly
          cp optimized-code.js lib/optimized-code.js.example
          git add lib/optimized-code.js.example
        fi
        
        # Add optimization report
        mkdir -p docs/performance
        cp optimization-report.md docs/performance/
        git add docs/performance/optimization-report.md
        
        # Commit and push
        git commit -m "🚀 Performance optimization suggestions

        Based on automated analysis:
        - Identified optimization opportunities
        - Generated optimized code examples
        - Documented expected improvements

        See optimization-report.md for details"
        
        git push origin $BRANCH
        
        # Create PR
        gh pr create \
          --title "🚀 Performance Optimizations - $(date +%B)" \
          --body "## Performance Optimization Analysis
          
          This PR contains optimization suggestions generated using free AI models.
          
          ### Summary:
          $(grep -A 3 "Expected performance gain" optimization-report.md | head -20)
          
          ### Files:
          - \`docs/performance/optimization-report.md\` - Full analysis
          - \`lib/optimized-code.js.example\` - Example optimized code
          
          ### Next Steps:
          1. Review the suggestions
          2. Implement the most impactful ones
          3. Benchmark before/after
          
          ---
          *Generated using GitHub Models/OpenRouter free AI models*" \
          --label "performance,enhancement"
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    - name: Benchmark Comparison
      env:
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      run: |
        # Use claude-yolt for benchmark analysis (eating our own dog food)
        if [ -n "$ANTHROPIC_API_KEY" ]; then
          mkdir -p ~/.config/@anthropic-ai/claude-code
          echo "{\"apiKey\": \"$ANTHROPIC_API_KEY\"}" > ~/.config/@anthropic-ai/claude-code/config.json
          echo "Using claude-yolt for benchmark comparison..."
          
          claude-yolt "Create a benchmark comparison report:
          
          Data:
          $(cat performance-baseline.txt)
          
          Generate:
          1. Performance comparison table
          2. Overhead percentage calculation
          3. Recommendations for different use cases
          4. Settings for optimal performance
          
          Format as markdown with tables" > benchmark-report.md 2>&1
        elif [ -n "$GITHUB_TOKEN" ]; then
          curl -s "https://models.inference.ai.azure.com/chat/completions" \
            -H "Authorization: Bearer $GITHUB_TOKEN" \
            -H "Content-Type: application/json" \
            -d '{
              "model": "gpt-4o-mini",
              "messages": [{
                "role": "user",
                "content": "Create a benchmark comparison report:\n\nData:\n'$(cat performance-baseline.txt | jq -Rs .)'\n\nGenerate:\n1. Performance comparison table\n2. Overhead percentage calculation\n3. Recommendations for different use cases\n4. Settings for optimal performance\n\nFormat as markdown with tables"
              }],
              "temperature": 0.1,
              "max_tokens": 1500
            }' | jq -r '.choices[0].message.content' > benchmark-report.md
        elif [ -n "$OPENROUTER_API_KEY" ]; then
          aider --model cohere/command-r-plus-08-2024:free \
                --message "Create benchmark comparison report: performance table, overhead percentages, recommendations, optimal settings" \
                --no-git --yes < performance-baseline.txt > benchmark-report.md
        else
          echo "## Benchmark Comparison" > benchmark-report.md
          echo "Report generation skipped - no API key available" >> benchmark-report.md
        fi
        
        # Create issue if significant overhead
        if grep -q "overhead: [5-9][0-9]%\|overhead: [1-9][0-9][0-9]%" benchmark-report.md; then
          gh issue create --title "⚠️ High performance overhead detected" \
            --body "$(cat benchmark-report.md)" \
            --label "performance,urgent"
        fi
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}